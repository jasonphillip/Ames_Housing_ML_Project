{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8522448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n\\n# the below extension properly formats a cell after it is run\\n%load_ext nb_black\\n\\n# Set the maximum number of rows to 200\\npd.set_option(\\\"display.max_rows\\\", 200)\\n\\n\\n# Set the maximum number of columns to 200\\npd.set_option(\\\"display.max_columns\\\", 200)\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n\\n# the below extension properly formats a cell after it is run\\n%load_ext nb_black\\n\\n# Set the maximum number of rows to 200\\npd.set_option(\\\"display.max_rows\\\", 200)\\n\\n\\n# Set the maximum number of columns to 200\\npd.set_option(\\\"display.max_columns\\\", 200)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# the below extension properly formats a cell after it is run\n",
    "%load_ext nb_black\n",
    "\n",
    "# Set the maximum number of rows to 200\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "\n",
    "# Set the maximum number of columns to 200\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d1428d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 28;\n",
       "                var nbb_unformatted_code = \"housing = pd.read_csv(\\\"../data/housing_corr.csv\\\")\";\n",
       "                var nbb_formatted_code = \"housing = pd.read_csv(\\\"../data/housing_corr.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing = pd.read_csv(\"../data/housing_corr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "541cbdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID : number of values 2558\n",
      "SalePrice : number of values 914\n",
      "MSSubClass : number of values 16\n",
      "MSZoning : number of values 6\n",
      "LotFrontage : number of values 127\n",
      "LotArea : number of values 1741\n",
      "Street : number of values 2\n",
      "Alley : number of values 3\n",
      "LotShape : number of values 4\n",
      "LandContour : number of values 4\n",
      "Utilities : number of values 2\n",
      "LotConfig : number of values 5\n",
      "LandSlope : number of values 3\n",
      "Neighborhood : number of values 26\n",
      "Condition1 : number of values 9\n",
      "Condition2 : number of values 8\n",
      "BldgType : number of values 5\n",
      "HouseStyle : number of values 8\n",
      "OverallQual : number of values 10\n",
      "OverallCond : number of values 9\n",
      "RoofStyle : number of values 6\n",
      "RoofMatl : number of values 7\n",
      "Exterior1st : number of values 15\n",
      "Exterior2nd : number of values 16\n",
      "MasVnrType : number of values 4\n",
      "MasVnrArea : number of values 414\n",
      "ExterCond : number of values 5\n",
      "Foundation : number of values 6\n",
      "BsmtQual : number of values 6\n",
      "BsmtCond : number of values 6\n",
      "BsmtExposure : number of values 5\n",
      "BsmtFinSF1 : number of values 924\n",
      "BsmtFinSF2 : number of values 262\n",
      "BsmtUnfSF : number of values 1057\n",
      "Heating : number of values 6\n",
      "HeatingQC : number of values 5\n",
      "CentralAir : number of values 2\n",
      "Electrical : number of values 4\n",
      "2ndFlrSF : number of values 589\n",
      "BsmtFullBath : number of values 4\n",
      "BsmtHalfBath : number of values 3\n",
      "HalfBath : number of values 3\n",
      "BedroomAbvGr : number of values 7\n",
      "KitchenAbvGr : number of values 4\n",
      "KitchenQual : number of values 5\n",
      "TotRmsAbvGrd : number of values 12\n",
      "Functional : number of values 7\n",
      "Fireplaces : number of values 5\n",
      "GarageType : number of values 8\n",
      "GarageFinish : number of values 4\n",
      "GarageArea : number of values 572\n",
      "GarageQual : number of values 6\n",
      "PavedDrive : number of values 3\n",
      "WoodDeckSF : number of values 370\n",
      "OpenPorchSF : number of values 237\n",
      "EnclosedPorch : number of values 169\n",
      "ScreenPorch : number of values 113\n",
      "Fence : number of values 5\n",
      "MoSold : number of values 12\n",
      "YrSold : number of values 5\n",
      "SaleType : number of values 10\n",
      "SaleCondition : number of values 6\n",
      "Age : number of values 126\n",
      "RemodAge : number of values 62\n",
      "TotalSF : number of values 1379\n",
      "Remodeled : number of values 2\n",
      "TotalPorchSF : number of values 352\n",
      "TotalBath : number of values 11\n",
      "MSSubClass_cat : number of values 16\n",
      "Street_type : number of values 12\n",
      "Neighborhood_st : number of values 501\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"for name in housing.columns:\\n    print(name, \\\": number of values\\\", len(housing[name].value_counts()))\";\n",
       "                var nbb_formatted_code = \"for name in housing.columns:\\n    print(name, \\\": number of values\\\", len(housing[name].value_counts()))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for name in housing.columns:\n",
    "    print(name, \": number of values\", len(housing[name].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ed62c3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 30;\n",
       "                var nbb_unformatted_code = \"housing.drop(\\n    [\\\"PID\\\", \\\"Neighborhood_st\\\"], axis=1, inplace=True,\\n)\";\n",
       "                var nbb_formatted_code = \"housing.drop(\\n    [\\\"PID\\\", \\\"Neighborhood_st\\\"], axis=1, inplace=True,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "housing.drop(\n",
    "    [\"PID\", \"Neighborhood_st\"], axis=1, inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed07722",
   "metadata": {},
   "source": [
    "## Oridnal Coding since this is the preferred method for Random Forest over Dummifying (One-Hot Encoding) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e04055ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 31;\n",
       "                var nbb_unformatted_code = \"# get a list of categorical features (i.e., object type columns)\\ncat_features = list(housing.select_dtypes(include=[\\\"object\\\"]).columns)\\ncat_features.remove(\\\"Neighborhood\\\")\";\n",
       "                var nbb_formatted_code = \"# get a list of categorical features (i.e., object type columns)\\ncat_features = list(housing.select_dtypes(include=[\\\"object\\\"]).columns)\\ncat_features.remove(\\\"Neighborhood\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of categorical features (i.e., object type columns)\n",
    "cat_features = list(housing.select_dtypes(include=[\"object\"]).columns)\n",
    "cat_features.remove(\"Neighborhood\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69572cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 32;\n",
       "                var nbb_unformatted_code = \"from sklearn.preprocessing import OrdinalEncoder\\n\\n# create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n\\n# fit the ordinal encoder to the categorical features\\nordinal_encoder.fit(housing[cat_features])\\n\\n# transform the categorical features into encoded numerical values\\nhousing[cat_features] = ordinal_encoder.transform(housing[cat_features])\";\n",
       "                var nbb_formatted_code = \"from sklearn.preprocessing import OrdinalEncoder\\n\\n# create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n\\n# fit the ordinal encoder to the categorical features\\nordinal_encoder.fit(housing[cat_features])\\n\\n# transform the categorical features into encoded numerical values\\nhousing[cat_features] = ordinal_encoder.transform(housing[cat_features])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# create an instance of the OrdinalEncoder class\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "\n",
    "# fit the ordinal encoder to the categorical features\n",
    "ordinal_encoder.fit(housing[cat_features])\n",
    "\n",
    "# transform the categorical features into encoded numerical values\n",
    "housing[cat_features] = ordinal_encoder.transform(housing[cat_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beaf22aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing neighborhood: SWISU\n",
      "         Feature  Importance\n",
      "35      2ndFlrSF    0.157702\n",
      "61       TotalSF    0.112373\n",
      "16   OverallCond    0.104800\n",
      "42  TotRmsAbvGrd    0.071526\n",
      "3        LotArea    0.070801\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Edwards\n",
      "       Feature  Importance\n",
      "61     TotalSF    0.380209\n",
      "3      LotArea    0.138225\n",
      "47  GarageArea    0.082139\n",
      "59         Age    0.053246\n",
      "60    RemodAge    0.031181\n",
      "\n",
      "\n",
      "Analyzing neighborhood: IDOTRR\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.355999\n",
      "15  OverallQual    0.207659\n",
      "60     RemodAge    0.073947\n",
      "49   PavedDrive    0.064591\n",
      "35     2ndFlrSF    0.054537\n",
      "\n",
      "\n",
      "Analyzing neighborhood: OldTown\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.351097\n",
      "35     2ndFlrSF    0.162087\n",
      "15  OverallQual    0.079209\n",
      "16  OverallCond    0.070586\n",
      "18     RoofMatl    0.039303\n",
      "\n",
      "\n",
      "Analyzing neighborhood: NWAmes\n",
      "          Feature  Importance\n",
      "61        TotalSF    0.672153\n",
      "35       2ndFlrSF    0.059292\n",
      "58  SaleCondition    0.033609\n",
      "47     GarageArea    0.032596\n",
      "42   TotRmsAbvGrd    0.027028\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Blmngtn\n",
      "        Feature  Importance\n",
      "56       YrSold    0.207322\n",
      "2   LotFrontage    0.193974\n",
      "30    BsmtUnfSF    0.183836\n",
      "55       MoSold    0.083419\n",
      "61      TotalSF    0.076794\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Mitchel\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.501223\n",
      "15  OverallQual    0.135241\n",
      "59          Age    0.036096\n",
      "64    TotalBath    0.027838\n",
      "51  OpenPorchSF    0.027750\n",
      "\n",
      "\n",
      "Analyzing neighborhood: NridgHt\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.654440\n",
      "28   BsmtFinSF1    0.146537\n",
      "22   MasVnrArea    0.034964\n",
      "47   GarageArea    0.022596\n",
      "15  OverallQual    0.017905\n",
      "\n",
      "\n",
      "Analyzing neighborhood: NAmes\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.419183\n",
      "3       LotArea    0.100513\n",
      "15  OverallQual    0.085638\n",
      "64    TotalBath    0.037937\n",
      "47   GarageArea    0.032218\n",
      "\n",
      "\n",
      "Analyzing neighborhood: CollgCr\n",
      "          Feature  Importance\n",
      "61        TotalSF    0.703084\n",
      "15    OverallQual    0.085085\n",
      "47     GarageArea    0.033526\n",
      "28     BsmtFinSF1    0.024317\n",
      "52  EnclosedPorch    0.016850\n",
      "\n",
      "\n",
      "Analyzing neighborhood: SawyerW\n",
      "      Feature  Importance\n",
      "61    TotalSF    0.760054\n",
      "59        Age    0.047108\n",
      "3     LotArea    0.028697\n",
      "64  TotalBath    0.026457\n",
      "30  BsmtUnfSF    0.012330\n",
      "\n",
      "\n",
      "Analyzing neighborhood: MeadowV\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.399390\n",
      "28   BsmtFinSF1    0.126225\n",
      "3       LotArea    0.108914\n",
      "60     RemodAge    0.061209\n",
      "2   LotFrontage    0.049264\n",
      "\n",
      "\n",
      "Analyzing neighborhood: BrDale\n",
      "         Feature  Importance\n",
      "61       TotalSF    0.357430\n",
      "22    MasVnrArea    0.087589\n",
      "30     BsmtUnfSF    0.067043\n",
      "35      2ndFlrSF    0.064774\n",
      "42  TotRmsAbvGrd    0.059186\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Gilbert\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.521068\n",
      "35     2ndFlrSF    0.125694\n",
      "47   GarageArea    0.064945\n",
      "15  OverallQual    0.043025\n",
      "3       LotArea    0.023562\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Timber\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.617546\n",
      "47   GarageArea    0.086444\n",
      "22   MasVnrArea    0.051870\n",
      "15  OverallQual    0.051822\n",
      "20  Exterior2nd    0.022467\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Somerst\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.534955\n",
      "28   BsmtFinSF1    0.098216\n",
      "41  KitchenQual    0.078601\n",
      "25     BsmtQual    0.061651\n",
      "3       LotArea    0.037633\n",
      "\n",
      "\n",
      "Analyzing neighborhood: BrkSide\n",
      "         Feature  Importance\n",
      "61       TotalSF    0.607962\n",
      "35      2ndFlrSF    0.109889\n",
      "3        LotArea    0.064702\n",
      "15   OverallQual    0.026595\n",
      "42  TotRmsAbvGrd    0.013818\n",
      "\n",
      "\n",
      "Analyzing neighborhood: NoRidge\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.479890\n",
      "47   GarageArea    0.093673\n",
      "15  OverallQual    0.080520\n",
      "35     2ndFlrSF    0.064926\n",
      "64    TotalBath    0.055009\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Sawyer\n",
      "       Feature  Importance\n",
      "44  Fireplaces    0.286176\n",
      "61     TotalSF    0.193477\n",
      "59         Age    0.078523\n",
      "47  GarageArea    0.043829\n",
      "28  BsmtFinSF1    0.040650\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Crawfor\n",
      "        Feature  Importance\n",
      "61      TotalSF    0.422262\n",
      "64    TotalBath    0.089762\n",
      "41  KitchenQual    0.060829\n",
      "16  OverallCond    0.053657\n",
      "15  OverallQual    0.048758\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Greens\n",
      "         Feature  Importance\n",
      "28    BsmtFinSF1    0.236831\n",
      "44    Fireplaces    0.157770\n",
      "63  TotalPorchSF    0.125591\n",
      "56        YrSold    0.100966\n",
      "51   OpenPorchSF    0.069324\n",
      "\n",
      "\n",
      "Analyzing neighborhood: ClearCr\n",
      "        Feature  Importance\n",
      "3       LotArea    0.256114\n",
      "15  OverallQual    0.122274\n",
      "61      TotalSF    0.107290\n",
      "28   BsmtFinSF1    0.091124\n",
      "60     RemodAge    0.077182\n",
      "\n",
      "\n",
      "Analyzing neighborhood: StoneBr\n",
      "       Feature  Importance\n",
      "61     TotalSF    0.279144\n",
      "22  MasVnrArea    0.258447\n",
      "3      LotArea    0.115525\n",
      "47  GarageArea    0.080469\n",
      "59         Age    0.053771\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Veenker\n",
      "         Feature  Importance\n",
      "36  BsmtFullBath    0.337233\n",
      "30     BsmtUnfSF    0.119369\n",
      "15   OverallQual    0.082000\n",
      "61       TotalSF    0.072746\n",
      "3        LotArea    0.064669\n",
      "\n",
      "\n",
      "Analyzing neighborhood: NPkVill\n",
      "         Feature  Importance\n",
      "42  TotRmsAbvGrd    0.243822\n",
      "30     BsmtUnfSF    0.079241\n",
      "50    WoodDeckSF    0.067031\n",
      "56        YrSold    0.047678\n",
      "61       TotalSF    0.039828\n",
      "\n",
      "\n",
      "Analyzing neighborhood: Blueste\n",
      "      Feature  Importance\n",
      "61    TotalSF    0.116431\n",
      "64  TotalBath    0.099439\n",
      "35   2ndFlrSF    0.072437\n",
      "3     LotArea    0.053633\n",
      "59        Age    0.050853\n",
      "\n",
      "\n",
      "Neighborhood mapping:\n",
      "{'Blmngtn': 0, 'Blueste': 1, 'BrDale': 2, 'BrkSide': 3, 'ClearCr': 4, 'CollgCr': 5, 'Crawfor': 6, 'Edwards': 7, 'Gilbert': 8, 'Greens': 9, 'IDOTRR': 10, 'MeadowV': 11, 'Mitchel': 12, 'NAmes': 13, 'NPkVill': 14, 'NWAmes': 15, 'NoRidge': 16, 'NridgHt': 17, 'OldTown': 18, 'SWISU': 19, 'Sawyer': 20, 'SawyerW': 21, 'Somerst': 22, 'StoneBr': 23, 'Timber': 24, 'Veenker': 25}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 33;\n",
       "                var nbb_unformatted_code = \"from sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n# Create a label encoder instance\\nlabel_encoder = LabelEncoder()\\n\\n# Encode the neighborhood column and store the mapping\\nhousing['NeighborhoodEncoded'] = label_encoder.fit_transform(housing['Neighborhood'])\\nneighborhood_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\\n\\n# Get unique neighborhoods\\nunique_neighborhoods = housing['Neighborhood'].unique()\\n\\n# Loop through each neighborhood\\nfor neighborhood in unique_neighborhoods:\\n    print(f\\\"Analyzing neighborhood: {neighborhood}\\\")\\n    \\n    # Filter data based on the neighborhood\\n    neighborhood_data = housing[housing['Neighborhood'] == neighborhood]\\n\\n    # Split data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(neighborhood_data.drop([\\\"SalePrice\\\", \\\"Neighborhood\\\", \\\"NeighborhoodEncoded\\\"], axis=1), neighborhood_data[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n    # Train the Random Forest model\\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\\n    rf.fit(X_train, y_train)\\n\\n    # Calculate feature importances\\n    feature_importances = rf.feature_importances_\\n\\n    # Analyze the feature importances\\n    important_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False).head(5)\\n\\n    print(important_features)\\n    print(\\\"\\\\n\\\")\\n\\n# Print the mapping between neighborhood names and encoded values\\nprint(\\\"Neighborhood mapping:\\\")\\nprint(neighborhood_mapping)\";\n",
       "                var nbb_formatted_code = \"from sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n# Create a label encoder instance\\nlabel_encoder = LabelEncoder()\\n\\n# Encode the neighborhood column and store the mapping\\nhousing[\\\"NeighborhoodEncoded\\\"] = label_encoder.fit_transform(housing[\\\"Neighborhood\\\"])\\nneighborhood_mapping = dict(\\n    zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))\\n)\\n\\n# Get unique neighborhoods\\nunique_neighborhoods = housing[\\\"Neighborhood\\\"].unique()\\n\\n# Loop through each neighborhood\\nfor neighborhood in unique_neighborhoods:\\n    print(f\\\"Analyzing neighborhood: {neighborhood}\\\")\\n\\n    # Filter data based on the neighborhood\\n    neighborhood_data = housing[housing[\\\"Neighborhood\\\"] == neighborhood]\\n\\n    # Split data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        neighborhood_data.drop(\\n            [\\\"SalePrice\\\", \\\"Neighborhood\\\", \\\"NeighborhoodEncoded\\\"], axis=1\\n        ),\\n        neighborhood_data[\\\"SalePrice\\\"],\\n        test_size=0.2,\\n        random_state=42,\\n    )\\n\\n    # Train the Random Forest model\\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\\n    rf.fit(X_train, y_train)\\n\\n    # Calculate feature importances\\n    feature_importances = rf.feature_importances_\\n\\n    # Analyze the feature importances\\n    important_features = (\\n        pd.DataFrame({\\\"Feature\\\": X_train.columns, \\\"Importance\\\": feature_importances})\\n        .sort_values(\\\"Importance\\\", ascending=False)\\n        .head(5)\\n    )\\n\\n    print(important_features)\\n    print(\\\"\\\\n\\\")\\n\\n# Print the mapping between neighborhood names and encoded values\\nprint(\\\"Neighborhood mapping:\\\")\\nprint(neighborhood_mapping)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder instance\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Encode the neighborhood column and store the mapping\n",
    "housing['NeighborhoodEncoded'] = label_encoder.fit_transform(housing['Neighborhood'])\n",
    "neighborhood_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "# Get unique neighborhoods\n",
    "unique_neighborhoods = housing['Neighborhood'].unique()\n",
    "\n",
    "# Loop through each neighborhood\n",
    "for neighborhood in unique_neighborhoods:\n",
    "    print(f\"Analyzing neighborhood: {neighborhood}\")\n",
    "    \n",
    "    # Filter data based on the neighborhood\n",
    "    neighborhood_data = housing[housing['Neighborhood'] == neighborhood]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(neighborhood_data.drop([\"SalePrice\", \"Neighborhood\", \"NeighborhoodEncoded\"], axis=1), neighborhood_data[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate feature importances\n",
    "    feature_importances = rf.feature_importances_\n",
    "\n",
    "    # Analyze the feature importances\n",
    "    important_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False).head(5)\n",
    "\n",
    "    print(important_features)\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print the mapping between neighborhood names and encoded values\n",
    "print(\"Neighborhood mapping:\")\n",
    "print(neighborhood_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a896b05",
   "metadata": {},
   "source": [
    "## Random Forest to determine each feature correlation with sale price ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2545bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Feature  Importance\n",
      "15          OverallQual    0.489539\n",
      "61              TotalSF    0.310159\n",
      "59                  Age    0.020941\n",
      "35             2ndFlrSF    0.016088\n",
      "41          KitchenQual    0.015583\n",
      "47           GarageArea    0.012832\n",
      "3               LotArea    0.011795\n",
      "60             RemodAge    0.011290\n",
      "64            TotalBath    0.011272\n",
      "28           BsmtFinSF1    0.010304\n",
      "30            BsmtUnfSF    0.009602\n",
      "16          OverallCond    0.006532\n",
      "44           Fireplaces    0.005701\n",
      "25             BsmtQual    0.004743\n",
      "63         TotalPorchSF    0.004146\n",
      "50           WoodDeckSF    0.003957\n",
      "67  NeighborhoodEncoded    0.003761\n",
      "2           LotFrontage    0.003724\n",
      "22           MasVnrArea    0.003724\n",
      "51          OpenPorchSF    0.003231\n",
      "55               MoSold    0.002918\n",
      "1              MSZoning    0.002534\n",
      "36         BsmtFullBath    0.001862\n",
      "65       MSSubClass_cat    0.001802\n",
      "45           GarageType    0.001731\n",
      "53          ScreenPorch    0.001588\n",
      "46         GarageFinish    0.001569\n",
      "42         TotRmsAbvGrd    0.001529\n",
      "66          Street_type    0.001494\n",
      "20          Exterior2nd    0.001367\n",
      "56               YrSold    0.001361\n",
      "19          Exterior1st    0.001347\n",
      "27         BsmtExposure    0.001157\n",
      "7           LandContour    0.001136\n",
      "33           CentralAir    0.001123\n",
      "39         BedroomAbvGr    0.001018\n",
      "0            MSSubClass    0.001001\n",
      "17            RoofStyle    0.000942\n",
      "32            HeatingQC    0.000915\n",
      "14           HouseStyle    0.000912\n",
      "52        EnclosedPorch    0.000859\n",
      "6              LotShape    0.000716\n",
      "38             HalfBath    0.000712\n",
      "57             SaleType    0.000708\n",
      "9             LotConfig    0.000696\n",
      "24           Foundation    0.000676\n",
      "21           MasVnrType    0.000622\n",
      "43           Functional    0.000617\n",
      "58        SaleCondition    0.000613\n",
      "49           PavedDrive    0.000588\n",
      "26             BsmtCond    0.000512\n",
      "11           Condition1    0.000499\n",
      "12           Condition2    0.000492\n",
      "62            Remodeled    0.000462\n",
      "48           GarageQual    0.000408\n",
      "54                Fence    0.000389\n",
      "13             BldgType    0.000381\n",
      "29           BsmtFinSF2    0.000342\n",
      "23            ExterCond    0.000333\n",
      "18             RoofMatl    0.000255\n",
      "40         KitchenAbvGr    0.000206\n",
      "37         BsmtHalfBath    0.000187\n",
      "5                 Alley    0.000181\n",
      "10            LandSlope    0.000156\n",
      "34           Electrical    0.000119\n",
      "31              Heating    0.000029\n",
      "4                Street    0.000010\n",
      "8             Utilities    0.000000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 36;\n",
       "                var nbb_unformatted_code = \"\\n\\n# Create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n# Encode the Neighborhood column\\nhousing['NeighborhoodEncoded'] = ordinal_encoder.fit_transform(housing[['Neighborhood']])\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(housing.drop([\\\"SalePrice\\\", \\\"Neighborhood\\\"], axis=1), housing[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Calculate feature importances\\nfeature_importances = rf.feature_importances_\\n\\n# Analyze the feature importances\\nimportant_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False)\\n\\nprint(important_features)\";\n",
       "                var nbb_formatted_code = \"# Create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n# Encode the Neighborhood column\\nhousing[\\\"NeighborhoodEncoded\\\"] = ordinal_encoder.fit_transform(\\n    housing[[\\\"Neighborhood\\\"]]\\n)\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    housing.drop([\\\"SalePrice\\\", \\\"Neighborhood\\\"], axis=1),\\n    housing[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Calculate feature importances\\nfeature_importances = rf.feature_importances_\\n\\n# Analyze the feature importances\\nimportant_features = pd.DataFrame(\\n    {\\\"Feature\\\": X_train.columns, \\\"Importance\\\": feature_importances}\\n).sort_values(\\\"Importance\\\", ascending=False)\\n\\nprint(important_features)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create an instance of the OrdinalEncoder class\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Encode the Neighborhood column\n",
    "housing['NeighborhoodEncoded'] = ordinal_encoder.fit_transform(housing[['Neighborhood']])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(housing.drop([\"SalePrice\", \"Neighborhood\"], axis=1), housing[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Analyze the feature importances\n",
    "important_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d89ae7",
   "metadata": {},
   "source": [
    "## Using Random Forest to Look at the most important feature for each street grouping ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bdf2aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"# Running throught the process from Creating_test_train notebook to create quantiles\\n\\n#Loading Dateset again\\nhousing = pd.read_csv(\\\"../data/housing_corr.csv\\\")\\n\\n\\n# Splitting the data into train test and stratifying on neighborhood since that is what we are intested in\\ntrain_set, test_set = train_test_split(\\n    housing, test_size=0.2, stratify=housing[\\\"Neighborhood\\\"], random_state=42\\n)\\n\\n\\ndef group_neighbor_streets_by_saleprice(\\n    traindf=train_set,\\n    testdf=test_set,\\n    num_quantiles=10,  # notice the difference in this line!!\\n):\\n    # Calculate the mean sale price for each street in the training df\\n    street_prices = traindf.groupby(\\\"Neighborhood_st\\\")[\\\"SalePrice\\\"].mean()\\n    # Group the streets into the specified number of quantiles based on sale price\\n    labels = [f\\\"group_{i+1}\\\" for i in range(num_quantiles)]\\n    groups = pd.qcut(street_prices, q=num_quantiles, labels=range(1, num_quantiles + 1))\\n    # Create a dictionary that maps each street name to its corresponding sale price group label\\n    street_group_dict = dict(zip(street_prices.index, groups))\\n    # Add a new column to the training dataframe with the street price groups\\n    traindf[\\\"StreetPriceGroup\\\"] = traindf[\\\"Neighborhood_st\\\"].map(street_group_dict)\\n    # Add a new column to the testing dataframe with the street price groups\\n    testdf[\\\"StreetPriceGroup\\\"] = testdf[\\\"Neighborhood_st\\\"].map(street_group_dict)\\n    return street_group_dict\\n\\n\\n# this will use the dictionary created to fill in the missing values in the test df with\\n# another group in the same neighborhood\\n\\n\\ndef fill_na(testdf=test_set, d={}):\\n    # Extract the first part of the string in the \\\"Neighborhood_st\\\" column\\n    testdf[\\\"Neighborhood_prefix\\\"] = testdf[\\\"Neighborhood_st\\\"].map(\\n        lambda x: x.split(\\\"_\\\")[0]\\n    )\\n    # Create a new dict that only contains the neighborhood\\n    new_dict = {k.split(\\\"_\\\")[0]: v for k, v in d.items()}\\n    # Create a list of PIDs with missing StreetPriceGroup values\\n    na_pid_list = testdf[testdf[\\\"StreetPriceGroup\\\"].isna()][\\\"PID\\\"].tolist()\\n    # Create a Boolean mask to filter the DataFrame\\n    mask = testdf[\\\"PID\\\"].isin(na_pid_list)\\n    # Apply the dictionary mapping only to the filtered rows\\n    testdf.loc[mask, \\\"StreetPriceGroup\\\"] = testdf[mask][\\\"Neighborhood_prefix\\\"].map(\\n        new_dict\\n    )\\n    # Drop the column since there is no more use for it\\n    testdf.drop(\\\"Neighborhood_prefix\\\", axis=1, inplace=True)\\n\\n\\n# the num of quantiles can be changed and it is assigned to d which is the dictionary that\\n# will be used to fill in the missing values\\n\\nd = group_neighbor_streets_by_saleprice(\\n    traindf=train_set, testdf=test_set, num_quantiles=10,\\n)\\n\\nfill_na(test_set, d)\\n\\n\\n# Remove the column that was used to create groupings\\n# train_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\\n# test_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\\n\\n\\n# overly complicated code to remove PID and move SalePrice to first column\\n# I need to do this in the future and save the csv's after so we dont have to do this each time\\ntrain_set = train_set.iloc[\\n    :,\\n    train_set.columns.tolist().index(\\\"SalePrice\\\") : (\\n        train_set.columns.tolist().index(\\\"SalePrice\\\") + 1\\n    ),\\n].join(train_set.drop(columns=[\\\"SalePrice\\\", \\\"PID\\\"]))\\n# same with test set\\ntest_set = test_set.iloc[\\n    :,\\n    test_set.columns.tolist().index(\\\"SalePrice\\\") : (\\n        test_set.columns.tolist().index(\\\"SalePrice\\\") + 1\\n    ),\\n].join(test_set.drop(columns=[\\\"SalePrice\\\", \\\"PID\\\"]))\";\n",
       "                var nbb_formatted_code = \"# Running throught the process from Creating_test_train notebook to create quantiles\\n\\n# Loading Dateset again\\nhousing = pd.read_csv(\\\"../data/housing_corr.csv\\\")\\n\\n\\n# Splitting the data into train test and stratifying on neighborhood since that is what we are intested in\\ntrain_set, test_set = train_test_split(\\n    housing, test_size=0.2, stratify=housing[\\\"Neighborhood\\\"], random_state=42\\n)\\n\\n\\ndef group_neighbor_streets_by_saleprice(\\n    traindf=train_set,\\n    testdf=test_set,\\n    num_quantiles=10,  # notice the difference in this line!!\\n):\\n    # Calculate the mean sale price for each street in the training df\\n    street_prices = traindf.groupby(\\\"Neighborhood_st\\\")[\\\"SalePrice\\\"].mean()\\n    # Group the streets into the specified number of quantiles based on sale price\\n    labels = [f\\\"group_{i+1}\\\" for i in range(num_quantiles)]\\n    groups = pd.qcut(street_prices, q=num_quantiles, labels=range(1, num_quantiles + 1))\\n    # Create a dictionary that maps each street name to its corresponding sale price group label\\n    street_group_dict = dict(zip(street_prices.index, groups))\\n    # Add a new column to the training dataframe with the street price groups\\n    traindf[\\\"StreetPriceGroup\\\"] = traindf[\\\"Neighborhood_st\\\"].map(street_group_dict)\\n    # Add a new column to the testing dataframe with the street price groups\\n    testdf[\\\"StreetPriceGroup\\\"] = testdf[\\\"Neighborhood_st\\\"].map(street_group_dict)\\n    return street_group_dict\\n\\n\\n# this will use the dictionary created to fill in the missing values in the test df with\\n# another group in the same neighborhood\\n\\n\\ndef fill_na(testdf=test_set, d={}):\\n    # Extract the first part of the string in the \\\"Neighborhood_st\\\" column\\n    testdf[\\\"Neighborhood_prefix\\\"] = testdf[\\\"Neighborhood_st\\\"].map(\\n        lambda x: x.split(\\\"_\\\")[0]\\n    )\\n    # Create a new dict that only contains the neighborhood\\n    new_dict = {k.split(\\\"_\\\")[0]: v for k, v in d.items()}\\n    # Create a list of PIDs with missing StreetPriceGroup values\\n    na_pid_list = testdf[testdf[\\\"StreetPriceGroup\\\"].isna()][\\\"PID\\\"].tolist()\\n    # Create a Boolean mask to filter the DataFrame\\n    mask = testdf[\\\"PID\\\"].isin(na_pid_list)\\n    # Apply the dictionary mapping only to the filtered rows\\n    testdf.loc[mask, \\\"StreetPriceGroup\\\"] = testdf[mask][\\\"Neighborhood_prefix\\\"].map(\\n        new_dict\\n    )\\n    # Drop the column since there is no more use for it\\n    testdf.drop(\\\"Neighborhood_prefix\\\", axis=1, inplace=True)\\n\\n\\n# the num of quantiles can be changed and it is assigned to d which is the dictionary that\\n# will be used to fill in the missing values\\n\\nd = group_neighbor_streets_by_saleprice(\\n    traindf=train_set, testdf=test_set, num_quantiles=10,\\n)\\n\\nfill_na(test_set, d)\\n\\n\\n# Remove the column that was used to create groupings\\n# train_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\\n# test_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\\n\\n\\n# overly complicated code to remove PID and move SalePrice to first column\\n# I need to do this in the future and save the csv's after so we dont have to do this each time\\ntrain_set = train_set.iloc[\\n    :,\\n    train_set.columns.tolist().index(\\\"SalePrice\\\") : (\\n        train_set.columns.tolist().index(\\\"SalePrice\\\") + 1\\n    ),\\n].join(train_set.drop(columns=[\\\"SalePrice\\\", \\\"PID\\\"]))\\n# same with test set\\ntest_set = test_set.iloc[\\n    :,\\n    test_set.columns.tolist().index(\\\"SalePrice\\\") : (\\n        test_set.columns.tolist().index(\\\"SalePrice\\\") + 1\\n    ),\\n].join(test_set.drop(columns=[\\\"SalePrice\\\", \\\"PID\\\"]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running throught the process from Creating_test_train notebook to create quantiles\n",
    "\n",
    "# Loading Dateset again\n",
    "housing = pd.read_csv(\"../data/housing_corr.csv\")\n",
    "\n",
    "\n",
    "# Splitting the data into train test and stratifying on neighborhood since that is what we are intested in\n",
    "train_set, test_set = train_test_split(\n",
    "    housing, test_size=0.2, stratify=housing[\"Neighborhood\"], random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "def group_neighbor_streets_by_saleprice(\n",
    "    traindf=train_set,\n",
    "    testdf=test_set,\n",
    "    num_quantiles=10,  # notice the difference in this line!!\n",
    "):\n",
    "    # Calculate the mean sale price for each street in the training df\n",
    "    street_prices = traindf.groupby(\"Neighborhood_st\")[\"SalePrice\"].mean()\n",
    "    # Group the streets into the specified number of quantiles based on sale price\n",
    "    labels = [f\"group_{i+1}\" for i in range(num_quantiles)]\n",
    "    groups = pd.qcut(street_prices, q=num_quantiles, labels=range(1, num_quantiles + 1))\n",
    "    # Create a dictionary that maps each street name to its corresponding sale price group label\n",
    "    street_group_dict = dict(zip(street_prices.index, groups))\n",
    "    # Add a new column to the training dataframe with the street price groups\n",
    "    traindf[\"StreetPriceGroup\"] = traindf[\"Neighborhood_st\"].map(street_group_dict)\n",
    "    # Add a new column to the testing dataframe with the street price groups\n",
    "    testdf[\"StreetPriceGroup\"] = testdf[\"Neighborhood_st\"].map(street_group_dict)\n",
    "    return street_group_dict\n",
    "\n",
    "\n",
    "# this will use the dictionary created to fill in the missing values in the test df with\n",
    "# another group in the same neighborhood\n",
    "\n",
    "\n",
    "def fill_na(testdf=test_set, d={}):\n",
    "    # Extract the first part of the string in the \"Neighborhood_st\" column\n",
    "    testdf[\"Neighborhood_prefix\"] = testdf[\"Neighborhood_st\"].map(\n",
    "        lambda x: x.split(\"_\")[0]\n",
    "    )\n",
    "    # Create a new dict that only contains the neighborhood\n",
    "    new_dict = {k.split(\"_\")[0]: v for k, v in d.items()}\n",
    "    # Create a list of PIDs with missing StreetPriceGroup values\n",
    "    na_pid_list = testdf[testdf[\"StreetPriceGroup\"].isna()][\"PID\"].tolist()\n",
    "    # Create a Boolean mask to filter the DataFrame\n",
    "    mask = testdf[\"PID\"].isin(na_pid_list)\n",
    "    # Apply the dictionary mapping only to the filtered rows\n",
    "    testdf.loc[mask, \"StreetPriceGroup\"] = testdf[mask][\"Neighborhood_prefix\"].map(\n",
    "        new_dict\n",
    "    )\n",
    "    # Drop the column since there is no more use for it\n",
    "    testdf.drop(\"Neighborhood_prefix\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# the num of quantiles can be changed and it is assigned to d which is the dictionary that\n",
    "# will be used to fill in the missing values\n",
    "\n",
    "d = group_neighbor_streets_by_saleprice(\n",
    "    traindf=train_set, testdf=test_set, num_quantiles=10,\n",
    ")\n",
    "\n",
    "fill_na(test_set, d)\n",
    "\n",
    "\n",
    "# Remove the column that was used to create groupings\n",
    "# train_set.drop(\"Neighborhood_st\", axis=1, inplace=True)\n",
    "# test_set.drop(\"Neighborhood_st\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# overly complicated code to remove PID and move SalePrice to first column\n",
    "# I need to do this in the future and save the csv's after so we dont have to do this each time\n",
    "train_set = train_set.iloc[\n",
    "    :,\n",
    "    train_set.columns.tolist().index(\"SalePrice\") : (\n",
    "        train_set.columns.tolist().index(\"SalePrice\") + 1\n",
    "    ),\n",
    "].join(train_set.drop(columns=[\"SalePrice\", \"PID\"]))\n",
    "# same with test set\n",
    "test_set = test_set.iloc[\n",
    "    :,\n",
    "    test_set.columns.tolist().index(\"SalePrice\") : (\n",
    "        test_set.columns.tolist().index(\"SalePrice\") + 1\n",
    "    ),\n",
    "].join(test_set.drop(columns=[\"SalePrice\", \"PID\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd31f156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 45;\n",
       "                var nbb_unformatted_code = \"# get a list of categorical features (i.e., object type columns)\\ncat_features = list(train_set.select_dtypes(include=[\\\"object\\\"]).columns)\\n\\n# create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n\\n# fit the ordinal encoder to the categorical features\\nordinal_encoder.fit(train_set[cat_features])\\n\\n# transform the categorical features into encoded numerical values\\ntrain_set[cat_features] = ordinal_encoder.transform(train_set[cat_features])\";\n",
       "                var nbb_formatted_code = \"# get a list of categorical features (i.e., object type columns)\\ncat_features = list(train_set.select_dtypes(include=[\\\"object\\\"]).columns)\\n\\n# create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n\\n# fit the ordinal encoder to the categorical features\\nordinal_encoder.fit(train_set[cat_features])\\n\\n# transform the categorical features into encoded numerical values\\ntrain_set[cat_features] = ordinal_encoder.transform(train_set[cat_features])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of categorical features (i.e., object type columns)\n",
    "cat_features = list(train_set.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "# create an instance of the OrdinalEncoder class\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "\n",
    "# fit the ordinal encoder to the categorical features\n",
    "ordinal_encoder.fit(train_set[cat_features])\n",
    "\n",
    "# transform the categorical features into encoded numerical values\n",
    "train_set[cat_features] = ordinal_encoder.transform(train_set[cat_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a40e22b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 55;\n",
       "                var nbb_unformatted_code = \"# get a list of categorical features (i.e., object type columns)\\ncat_features = list(test_set.select_dtypes(include=[\\\"object\\\"]).columns)\\n\\n# create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n\\n# fit the ordinal encoder to the categorical features\\nordinal_encoder.fit(test_set[cat_features])\\n\\n# transform the categorical features into encoded numerical values\\ntest_set[cat_features] = ordinal_encoder.transform(test_set[cat_features])\";\n",
       "                var nbb_formatted_code = \"# get a list of categorical features (i.e., object type columns)\\ncat_features = list(test_set.select_dtypes(include=[\\\"object\\\"]).columns)\\n\\n# create an instance of the OrdinalEncoder class\\nordinal_encoder = OrdinalEncoder()\\n\\n\\n# fit the ordinal encoder to the categorical features\\nordinal_encoder.fit(test_set[cat_features])\\n\\n# transform the categorical features into encoded numerical values\\ntest_set[cat_features] = ordinal_encoder.transform(test_set[cat_features])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get a list of categorical features (i.e., object type columns)\n",
    "cat_features = list(test_set.select_dtypes(include=[\"object\"]).columns)\n",
    "\n",
    "# create an instance of the OrdinalEncoder class\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "\n",
    "# fit the ordinal encoder to the categorical features\n",
    "ordinal_encoder.fit(test_set[cat_features])\n",
    "\n",
    "# transform the categorical features into encoded numerical values\n",
    "test_set[cat_features] = ordinal_encoder.transform(test_set[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3dc91f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"# Remove the column that was used to create groupings\\ntrain_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\\ntest_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\";\n",
       "                var nbb_formatted_code = \"# Remove the column that was used to create groupings\\ntrain_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\\ntest_set.drop(\\\"Neighborhood_st\\\", axis=1, inplace=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Remove the column that was used to create groupings\n",
    "train_set.drop(\"Neighborhood_st\", axis=1, inplace=True)\n",
    "test_set.drop(\"Neighborhood_st\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9944f731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing group: 1\n",
      "Mean SalePrice: 96447.10714285714\n",
      "          Feature  Importance\n",
      "62        TotalSF    0.309368\n",
      "16    OverallQual    0.198242\n",
      "17    OverallCond    0.075767\n",
      "59  SaleCondition    0.040282\n",
      "61       RemodAge    0.035571\n",
      "\n",
      "\n",
      "Analyzing group: 2\n",
      "Mean SalePrice: 120687.70319634704\n",
      "        Feature  Importance\n",
      "62      TotalSF    0.365843\n",
      "34   CentralAir    0.062086\n",
      "17  OverallCond    0.057945\n",
      "31    BsmtUnfSF    0.053413\n",
      "61     RemodAge    0.044905\n",
      "\n",
      "\n",
      "Analyzing group: 3\n",
      "Mean SalePrice: 131822.11219512194\n",
      "        Feature  Importance\n",
      "16  OverallQual    0.225205\n",
      "62      TotalSF    0.163845\n",
      "48   GarageArea    0.087980\n",
      "36     2ndFlrSF    0.050391\n",
      "29   BsmtFinSF1    0.049707\n",
      "\n",
      "\n",
      "Analyzing group: 4\n",
      "Mean SalePrice: 141645.2448275862\n",
      "       Feature  Importance\n",
      "62     TotalSF    0.326022\n",
      "36    2ndFlrSF    0.105220\n",
      "60         Age    0.073954\n",
      "29  BsmtFinSF1    0.047697\n",
      "51  WoodDeckSF    0.044348\n",
      "\n",
      "\n",
      "Analyzing group: 5\n",
      "Mean SalePrice: 153341.14736842105\n",
      "       Feature  Importance\n",
      "62     TotalSF    0.234081\n",
      "48  GarageArea    0.144945\n",
      "61    RemodAge    0.062891\n",
      "29  BsmtFinSF1    0.056131\n",
      "3      LotArea    0.054758\n",
      "\n",
      "\n",
      "Analyzing group: 6\n",
      "Mean SalePrice: 172623.0892857143\n",
      "        Feature  Importance\n",
      "62      TotalSF    0.210328\n",
      "36     2ndFlrSF    0.156529\n",
      "65    TotalBath    0.068431\n",
      "60          Age    0.059520\n",
      "52  OpenPorchSF    0.057899\n",
      "\n",
      "\n",
      "Analyzing group: 7\n",
      "Mean SalePrice: 194234.9829351536\n",
      "        Feature  Importance\n",
      "62      TotalSF    0.313043\n",
      "16  OverallQual    0.151768\n",
      "36     2ndFlrSF    0.095263\n",
      "48   GarageArea    0.051317\n",
      "65    TotalBath    0.042815\n",
      "\n",
      "\n",
      "Analyzing group: 8\n",
      "Mean SalePrice: 219793.4143646409\n",
      "        Feature  Importance\n",
      "62      TotalSF    0.448103\n",
      "61     RemodAge    0.072096\n",
      "31    BsmtUnfSF    0.046296\n",
      "48   GarageArea    0.035628\n",
      "16  OverallQual    0.034020\n",
      "\n",
      "\n",
      "Analyzing group: 9\n",
      "Mean SalePrice: 260242.51685393258\n",
      "        Feature  Importance\n",
      "62      TotalSF    0.413600\n",
      "61     RemodAge    0.047872\n",
      "42  KitchenQual    0.046442\n",
      "48   GarageArea    0.040537\n",
      "51   WoodDeckSF    0.033673\n",
      "\n",
      "\n",
      "Analyzing group: 10\n",
      "Mean SalePrice: 349084.8051948052\n",
      "        Feature  Importance\n",
      "62      TotalSF    0.574323\n",
      "29   BsmtFinSF1    0.065110\n",
      "16  OverallQual    0.056232\n",
      "65    TotalBath    0.034824\n",
      "48   GarageArea    0.020903\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 48;\n",
       "                var nbb_unformatted_code = \"from sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\n\\n# Sort unique values in the StreetPriceGroup column by mean SalePrice in ascending order\\ngroup_means = train_set.groupby('StreetPriceGroup')['SalePrice'].mean()\\nunique_groups = group_means.sort_values().index\\n\\n# Loop through each value in the StreetPriceGroup column\\nfor group in unique_groups:\\n    print(f\\\"Analyzing group: {group}\\\")\\n    \\n    # Filter data based on the StreetPriceGroup value\\n    group_data = train_set[train_set['StreetPriceGroup'] == group]\\n\\n    # Print mean SalePrice for the group\\n    print(f\\\"Mean SalePrice: {group_data['SalePrice'].mean()}\\\")\\n\\n    # Split data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(group_data.drop([\\\"SalePrice\\\", \\\"StreetPriceGroup\\\"], axis=1), group_data[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n    # Train the Random Forest model\\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\\n    rf.fit(X_train, y_train)\\n\\n    # Calculate feature importances\\n    feature_importances = rf.feature_importances_\\n\\n    # Analyze the feature importances\\n    important_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False).head(5)\\n\\n    print(important_features)\\n    print(\\\"\\\\n\\\")\";\n",
       "                var nbb_formatted_code = \"from sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import train_test_split\\n\\n# Sort unique values in the StreetPriceGroup column by mean SalePrice in ascending order\\ngroup_means = train_set.groupby(\\\"StreetPriceGroup\\\")[\\\"SalePrice\\\"].mean()\\nunique_groups = group_means.sort_values().index\\n\\n# Loop through each value in the StreetPriceGroup column\\nfor group in unique_groups:\\n    print(f\\\"Analyzing group: {group}\\\")\\n\\n    # Filter data based on the StreetPriceGroup value\\n    group_data = train_set[train_set[\\\"StreetPriceGroup\\\"] == group]\\n\\n    # Print mean SalePrice for the group\\n    print(f\\\"Mean SalePrice: {group_data['SalePrice'].mean()}\\\")\\n\\n    # Split data into training and testing sets\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        group_data.drop([\\\"SalePrice\\\", \\\"StreetPriceGroup\\\"], axis=1),\\n        group_data[\\\"SalePrice\\\"],\\n        test_size=0.2,\\n        random_state=42,\\n    )\\n\\n    # Train the Random Forest model\\n    rf = RandomForestRegressor(n_estimators=100, random_state=42)\\n    rf.fit(X_train, y_train)\\n\\n    # Calculate feature importances\\n    feature_importances = rf.feature_importances_\\n\\n    # Analyze the feature importances\\n    important_features = (\\n        pd.DataFrame({\\\"Feature\\\": X_train.columns, \\\"Importance\\\": feature_importances})\\n        .sort_values(\\\"Importance\\\", ascending=False)\\n        .head(5)\\n    )\\n\\n    print(important_features)\\n    print(\\\"\\\\n\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sort unique values in the StreetPriceGroup column by mean SalePrice in ascending order\n",
    "group_means = train_set.groupby('StreetPriceGroup')['SalePrice'].mean()\n",
    "unique_groups = group_means.sort_values().index\n",
    "\n",
    "# Loop through each value in the StreetPriceGroup column\n",
    "for group in unique_groups:\n",
    "    print(f\"Analyzing group: {group}\")\n",
    "    \n",
    "    # Filter data based on the StreetPriceGroup value\n",
    "    group_data = train_set[train_set['StreetPriceGroup'] == group]\n",
    "\n",
    "    # Print mean SalePrice for the group\n",
    "    print(f\"Mean SalePrice: {group_data['SalePrice'].mean()}\")\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(group_data.drop([\"SalePrice\", \"StreetPriceGroup\"], axis=1), group_data[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate feature importances\n",
    "    feature_importances = rf.feature_importances_\n",
    "\n",
    "    # Analyze the feature importances\n",
    "    important_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False).head(5)\n",
    "\n",
    "    print(important_features)\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96595e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at the feature importance on SalePrice when adding the StreetPriceGroup variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "21d20a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Importance\n",
      "68  StreetPriceGroup    0.658141\n",
      "62           TotalSF    0.180576\n",
      "16       OverallQual    0.033746\n",
      "26          BsmtQual    0.009102\n",
      "29        BsmtFinSF1    0.008874\n",
      "48        GarageArea    0.008305\n",
      "36          2ndFlrSF    0.008026\n",
      "61          RemodAge    0.007759\n",
      "23        MasVnrArea    0.007466\n",
      "3            LotArea    0.006697\n",
      "65         TotalBath    0.006286\n",
      "31         BsmtUnfSF    0.005421\n",
      "60               Age    0.005041\n",
      "17       OverallCond    0.004357\n",
      "64      TotalPorchSF    0.003726\n",
      "51        WoodDeckSF    0.003237\n",
      "42       KitchenQual    0.002851\n",
      "2        LotFrontage    0.002566\n",
      "54       ScreenPorch    0.002385\n",
      "52       OpenPorchSF    0.002282\n",
      "43      TotRmsAbvGrd    0.002102\n",
      "56            MoSold    0.001971\n",
      "11      Neighborhood    0.001753\n",
      "49        GarageQual    0.001693\n",
      "47      GarageFinish    0.001394\n",
      "21       Exterior2nd    0.001322\n",
      "40      BedroomAbvGr    0.001320\n",
      "19          RoofMatl    0.001213\n",
      "57            YrSold    0.001191\n",
      "45        Fireplaces    0.001164\n",
      "37      BsmtFullBath    0.001151\n",
      "67       Street_type    0.001145\n",
      "20       Exterior1st    0.001090\n",
      "59     SaleCondition    0.000927\n",
      "12        Condition1    0.000922\n",
      "33         HeatingQC    0.000892\n",
      "28      BsmtExposure    0.000855\n",
      "66    MSSubClass_cat    0.000729\n",
      "0         MSSubClass    0.000695\n",
      "53     EnclosedPorch    0.000568\n",
      "58          SaleType    0.000558\n",
      "34        CentralAir    0.000552\n",
      "9          LotConfig    0.000546\n",
      "18         RoofStyle    0.000536\n",
      "39          HalfBath    0.000515\n",
      "7        LandContour    0.000486\n",
      "15        HouseStyle    0.000451\n",
      "22        MasVnrType    0.000424\n",
      "1           MSZoning    0.000407\n",
      "13        Condition2    0.000404\n",
      "55             Fence    0.000393\n",
      "46        GarageType    0.000387\n",
      "6           LotShape    0.000387\n",
      "38      BsmtHalfBath    0.000375\n",
      "50        PavedDrive    0.000353\n",
      "63         Remodeled    0.000339\n",
      "30        BsmtFinSF2    0.000331\n",
      "24         ExterCond    0.000313\n",
      "27          BsmtCond    0.000269\n",
      "25        Foundation    0.000232\n",
      "35        Electrical    0.000197\n",
      "44        Functional    0.000174\n",
      "14          BldgType    0.000156\n",
      "5              Alley    0.000098\n",
      "41      KitchenAbvGr    0.000096\n",
      "10         LandSlope    0.000069\n",
      "32           Heating    0.000013\n",
      "4             Street    0.000004\n",
      "8          Utilities    0.000000\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 54;\n",
       "                var nbb_unformatted_code = \"# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set.drop(\\\"SalePrice\\\", axis=1), train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Calculate feature importances\\nfeature_importances = rf.feature_importances_\\n\\n# Analyze the feature importances\\nimportant_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False)\\n\\nprint(important_features)\";\n",
       "                var nbb_formatted_code = \"# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set.drop(\\\"SalePrice\\\", axis=1),\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Calculate feature importances\\nfeature_importances = rf.feature_importances_\\n\\n# Analyze the feature importances\\nimportant_features = pd.DataFrame(\\n    {\\\"Feature\\\": X_train.columns, \\\"Importance\\\": feature_importances}\\n).sort_values(\\\"Importance\\\", ascending=False)\\n\\nprint(important_features)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set.drop(\"SalePrice\", axis=1), train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Analyze the feature importances\n",
    "important_features = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importances}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(important_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015fe1b4",
   "metadata": {},
   "source": [
    "## Using the top 20 features from above to predict SalePrice ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2673307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 14018.455731707316\n",
      "R-squared score: 0.9160173334216787\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 61;\n",
       "                var nbb_unformatted_code = \"from sklearn.metrics import mean_absolute_error, r2_score\\n\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set[important_features['Feature'][:20].tolist()], train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Predict SalePrice using the test data\\ny_pred = rf.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"from sklearn.metrics import mean_absolute_error, r2_score\\n\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set[important_features[\\\"Feature\\\"][:20].tolist()],\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Predict SalePrice using the test data\\ny_pred = rf.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set[important_features['Feature'][:20].tolist()], train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict SalePrice using the test data\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) and R-squared score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared score:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915172a9",
   "metadata": {},
   "source": [
    "## Trying again with bagging regressor ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f1a1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 13902.682904878047\n",
      "R-squared score: 0.916587279890754\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 63;\n",
       "                var nbb_unformatted_code = \"# from sklearn.ensemble import BaggingRegressor\\n# from sklearn.ensemble import RandomForestRegressor\\n# from sklearn.metrics import mean_absolute_error, r2_score\\n# from sklearn.model_selection import train_test_split\\n\\n\\n# Select the top 20 features\\ntop_20_features = important_features.head(20)[\\\"Feature\\\"].tolist()\\n\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set[top_20_features], train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Create and fit the Bagging Regressor with a Random Forest base estimator\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nbagging_rf = BaggingRegressor(base_estimator=rf, n_estimators=10, random_state=42)\\nbagging_rf.fit(X_train, y_train)\\n\\n# Predict SalePrice on the testing set\\ny_pred = bagging_rf.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# from sklearn.ensemble import BaggingRegressor\\n# from sklearn.ensemble import RandomForestRegressor\\n# from sklearn.metrics import mean_absolute_error, r2_score\\n# from sklearn.model_selection import train_test_split\\n\\n\\n# Select the top 20 features\\ntop_20_features = important_features.head(20)[\\\"Feature\\\"].tolist()\\n\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set[top_20_features], train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42\\n)\\n\\n# Create and fit the Bagging Regressor with a Random Forest base estimator\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nbagging_rf = BaggingRegressor(base_estimator=rf, n_estimators=10, random_state=42)\\nbagging_rf.fit(X_train, y_train)\\n\\n# Predict SalePrice on the testing set\\ny_pred = bagging_rf.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Select the top 20 features\n",
    "top_20_features = important_features.head(20)[\"Feature\"].tolist()\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set[top_20_features], train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the Bagging Regressor with a Random Forest base estimator\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "bagging_rf = BaggingRegressor(base_estimator=rf, n_estimators=10, random_state=42)\n",
    "bagging_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict SalePrice on the testing set\n",
    "y_pred = bagging_rf.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) and R-squared score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9bba4",
   "metadata": {},
   "source": [
    "## Now trying Stacking and Boosting with top 20 features ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90a57013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor Results:\n",
      "Mean Absolute Error: 15146.804259213986\n",
      "R-squared score: 0.9029411144822934\n",
      "\n",
      "Boosting Regressor Results:\n",
      "Mean Absolute Error: 13328.839803176925\n",
      "R-squared score: 0.9290692109057372\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 66;\n",
       "                var nbb_unformatted_code = \"# import pandas as pd\\n# import numpy as np\\nfrom sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\\nfrom sklearn.tree import DecisionTreeRegressor\\n# from sklearn.metrics import mean_absolute_error, r2_score\\n# from sklearn.model_selection import train_test_split\\n\\n# Select the top 20 features\\ntop_20_features = important_features['Feature'][:20].tolist()\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set[top_20_features], train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Define base estimators\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\ndt = DecisionTreeRegressor(random_state=42)\\n\\n# Create the stacking regressor using the base estimators\\nestimators = [('rf', rf), ('dt', dt)]\\nstack_reg = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42))\\n\\n# Fit the stacking regressor to the training data\\nstack_reg.fit(X_train, y_train)\\n\\n# Predict using the stacking regressor\\ny_pred_stack = stack_reg.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae_stack = mean_absolute_error(y_test, y_pred_stack)\\nr2_stack = r2_score(y_test, y_pred_stack)\\n\\nprint(\\\"Stacking Regressor Results:\\\")\\nprint(\\\"Mean Absolute Error:\\\", mae_stack)\\nprint(\\\"R-squared score:\\\", r2_stack)\\n\\n# Create the boosting regressor using the base estimator\\nboost_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\\n\\n# Fit the boosting regressor to the training data\\nboost_reg.fit(X_train, y_train)\\n\\n# Predict using the boosting regressor\\ny_pred_boost = boost_reg.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae_boost = mean_absolute_error(y_test, y_pred_boost)\\nr2_boost = r2_score(y_test, y_pred_boost)\\n\\nprint(\\\"\\\\nBoosting Regressor Results:\\\")\\nprint(\\\"Mean Absolute Error:\\\", mae_boost)\\nprint(\\\"R-squared score:\\\", r2_boost)\";\n",
       "                var nbb_formatted_code = \"# import pandas as pd\\n# import numpy as np\\nfrom sklearn.ensemble import (\\n    GradientBoostingRegressor,\\n    StackingRegressor,\\n    RandomForestRegressor,\\n)\\nfrom sklearn.tree import DecisionTreeRegressor\\n\\n# from sklearn.metrics import mean_absolute_error, r2_score\\n# from sklearn.model_selection import train_test_split\\n\\n# Select the top 20 features\\ntop_20_features = important_features[\\\"Feature\\\"][:20].tolist()\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set[top_20_features], train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42\\n)\\n\\n# Define base estimators\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\ndt = DecisionTreeRegressor(random_state=42)\\n\\n# Create the stacking regressor using the base estimators\\nestimators = [(\\\"rf\\\", rf), (\\\"dt\\\", dt)]\\nstack_reg = StackingRegressor(\\n    estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42)\\n)\\n\\n# Fit the stacking regressor to the training data\\nstack_reg.fit(X_train, y_train)\\n\\n# Predict using the stacking regressor\\ny_pred_stack = stack_reg.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae_stack = mean_absolute_error(y_test, y_pred_stack)\\nr2_stack = r2_score(y_test, y_pred_stack)\\n\\nprint(\\\"Stacking Regressor Results:\\\")\\nprint(\\\"Mean Absolute Error:\\\", mae_stack)\\nprint(\\\"R-squared score:\\\", r2_stack)\\n\\n# Create the boosting regressor using the base estimator\\nboost_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\\n\\n# Fit the boosting regressor to the training data\\nboost_reg.fit(X_train, y_train)\\n\\n# Predict using the boosting regressor\\ny_pred_boost = boost_reg.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae_boost = mean_absolute_error(y_test, y_pred_boost)\\nr2_boost = r2_score(y_test, y_pred_boost)\\n\\nprint(\\\"\\\\nBoosting Regressor Results:\\\")\\nprint(\\\"Mean Absolute Error:\\\", mae_boost)\\nprint(\\\"R-squared score:\\\", r2_boost)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Select the top 20 features\n",
    "top_20_features = important_features['Feature'][:20].tolist()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set[top_20_features], train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define base estimators\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "dt = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# Create the stacking regressor using the base estimators\n",
    "estimators = [('rf', rf), ('dt', dt)]\n",
    "stack_reg = StackingRegressor(estimators=estimators, final_estimator=GradientBoostingRegressor(random_state=42))\n",
    "\n",
    "# Fit the stacking regressor to the training data\n",
    "stack_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the stacking regressor\n",
    "y_pred_stack = stack_reg.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) and R-squared score\n",
    "mae_stack = mean_absolute_error(y_test, y_pred_stack)\n",
    "r2_stack = r2_score(y_test, y_pred_stack)\n",
    "\n",
    "print(\"Stacking Regressor Results:\")\n",
    "print(\"Mean Absolute Error:\", mae_stack)\n",
    "print(\"R-squared score:\", r2_stack)\n",
    "\n",
    "# Create the boosting regressor using the base estimator\n",
    "boost_reg = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the boosting regressor to the training data\n",
    "boost_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the boosting regressor\n",
    "y_pred_boost = boost_reg.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) and R-squared score\n",
    "mae_boost = mean_absolute_error(y_test, y_pred_boost)\n",
    "r2_boost = r2_score(y_test, y_pred_boost)\n",
    "\n",
    "print(\"\\nBoosting Regressor Results:\")\n",
    "print(\"Mean Absolute Error:\", mae_boost)\n",
    "print(\"R-squared score:\", r2_boost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f0032d",
   "metadata": {},
   "source": [
    "## Seeing if there is a difference when using all features and not top 20 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fedb13df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 14087.587317073172\n",
      "R-squared score: 0.9141713429944978\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 67;\n",
       "                var nbb_unformatted_code = \"\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set.drop(\\\"SalePrice\\\", axis=1), train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Make predictions on the test set\\ny_pred = rf.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set.drop(\\\"SalePrice\\\", axis=1),\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Train the Random Forest model\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nrf.fit(X_train, y_train)\\n\\n# Make predictions on the test set\\ny_pred = rf.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forestbbb\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set.drop(\"SalePrice\", axis=1), train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) and R-squared score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0640b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging - Mean Absolute Error: 13996.095329268293\n",
      "Bagging - R-squared score: 0.913565262760414\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 68;\n",
       "                var nbb_unformatted_code = \"# Bagging\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set.drop(\\\"SalePrice\\\", axis=1), train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Create and fit the Bagging Regressor with a Random Forest base estimator\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nbagging = BaggingRegressor(base_estimator=rf, n_estimators=10, random_state=42)\\nbagging.fit(X_train, y_train)\\n\\n# Make predictions and calculate scores\\ny_pred = bagging.predict(X_test)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"Bagging - Mean Absolute Error:\\\", mae)\\nprint(\\\"Bagging - R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Bagging\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set.drop(\\\"SalePrice\\\", axis=1),\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Create and fit the Bagging Regressor with a Random Forest base estimator\\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\\nbagging = BaggingRegressor(base_estimator=rf, n_estimators=10, random_state=42)\\nbagging.fit(X_train, y_train)\\n\\n# Make predictions and calculate scores\\ny_pred = bagging.predict(X_test)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"Bagging - Mean Absolute Error:\\\", mae)\\nprint(\\\"Bagging - R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bagging\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set.drop(\"SalePrice\", axis=1), train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the Bagging Regressor with a Random Forest base estimator\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "bagging = BaggingRegressor(base_estimator=rf, n_estimators=10, random_state=42)\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate scores\n",
    "y_pred = bagging.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Bagging - Mean Absolute Error:\", mae)\n",
    "print(\"Bagging - R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9fcb0983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking - Mean Absolute Error: 14067.524329716342\n",
      "Stacking - R-squared score: 0.9142560891429595\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 70;\n",
       "                var nbb_unformatted_code = \"# Stacking\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.svm import SVR\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set.drop(\\\"SalePrice\\\", axis=1), train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Create base estimators\\nestimators = [\\n    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\\n    ('knn', KNeighborsRegressor()),\\n    ('svr', SVR()),\\n]\\n\\n# Create and fit the Stacking Regressor with a Linear Regression final estimator\\nstacking = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\\nstacking.fit(X_train, y_train)\\n\\n# Make predictions and calculate scores\\ny_pred = stacking.predict(X_test)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"Stacking - Mean Absolute Error:\\\", mae)\\nprint(\\\"Stacking - R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Stacking\\n\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.svm import SVR\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set.drop(\\\"SalePrice\\\", axis=1),\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Create base estimators\\nestimators = [\\n    (\\\"rf\\\", RandomForestRegressor(n_estimators=100, random_state=42)),\\n    (\\\"knn\\\", KNeighborsRegressor()),\\n    (\\\"svr\\\", SVR()),\\n]\\n\\n# Create and fit the Stacking Regressor with a Linear Regression final estimator\\nstacking = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\\nstacking.fit(X_train, y_train)\\n\\n# Make predictions and calculate scores\\ny_pred = stacking.predict(X_test)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"Stacking - Mean Absolute Error:\\\", mae)\\nprint(\\\"Stacking - R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stacking\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set.drop(\"SalePrice\", axis=1), train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create base estimators\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('knn', KNeighborsRegressor()),\n",
    "    ('svr', SVR()),\n",
    "]\n",
    "\n",
    "# Create and fit the Stacking Regressor with a Linear Regression final estimator\n",
    "stacking = StackingRegressor(estimators=estimators, final_estimator=LinearRegression())\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate scores\n",
    "y_pred = stacking.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Stacking - Mean Absolute Error:\", mae)\n",
    "print(\"Stacking - R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3fe010cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting - Mean Absolute Error: 12704.623040466531\n",
      "Boosting - R-squared score: 0.935640719186301\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 71;\n",
       "                var nbb_unformatted_code = \"# Boosting\\n\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set.drop(\\\"SalePrice\\\", axis=1), train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Create and fit the Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions and calculate scores\\ny_pred = gbr.predict(X_test)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"Boosting - Mean Absolute Error:\\\", mae)\\nprint(\\\"Boosting - R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Boosting\\n\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set.drop(\\\"SalePrice\\\", axis=1),\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Create and fit the Gradient Boosting Regressor\\ngbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\\ngbr.fit(X_train, y_train)\\n\\n# Make predictions and calculate scores\\ny_pred = gbr.predict(X_test)\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(\\\"Boosting - Mean Absolute Error:\\\", mae)\\nprint(\\\"Boosting - R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Boosting\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_set.drop(\"SalePrice\", axis=1),\n",
    "    train_set[\"SalePrice\"],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create and fit the Gradient Boosting Regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and calculate scores\n",
    "y_pred = gbr.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Boosting - Mean Absolute Error:\", mae)\n",
    "print(\"Boosting - R-squared score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943ad83",
   "metadata": {},
   "source": [
    "## XGBoost ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24f2988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - Mean Absolute Error: 12184.325304878048\n",
      "XGBoost - R-squared score: 0.9370510881429757\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 75;\n",
       "                var nbb_unformatted_code = \"# Import necessary libraries\\nimport xgboost as xgb\\nfrom sklearn.metrics import mean_absolute_error, r2_score\\nfrom sklearn.model_selection import train_test_split\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(train_set.drop(\\\"SalePrice\\\", axis=1), train_set[\\\"SalePrice\\\"], test_size=0.2, random_state=42)\\n\\n# Create and fit the XGBoost model\\nxgb_model = xgb.XGBRegressor(objective=\\\"reg:squarederror\\\", random_state=42)\\nxgb_model.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = xgb_model.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"XGBoost - Mean Absolute Error:\\\", mae)\\nprint(\\\"XGBoost - R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"# Import necessary libraries\\nimport xgboost as xgb\\nfrom sklearn.metrics import mean_absolute_error, r2_score\\nfrom sklearn.model_selection import train_test_split\\n\\n# Split data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(\\n    train_set.drop(\\\"SalePrice\\\", axis=1),\\n    train_set[\\\"SalePrice\\\"],\\n    test_size=0.2,\\n    random_state=42,\\n)\\n\\n# Create and fit the XGBoost model\\nxgb_model = xgb.XGBRegressor(objective=\\\"reg:squarederror\\\", random_state=42)\\nxgb_model.fit(X_train, y_train)\\n\\n# Make predictions on the testing set\\ny_pred = xgb_model.predict(X_test)\\n\\n# Calculate Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\nprint(\\\"XGBoost - Mean Absolute Error:\\\", mae)\\nprint(\\\"XGBoost - R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_set.drop(\"SalePrice\", axis=1), train_set[\"SalePrice\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) and R-squared score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"XGBoost - Mean Absolute Error:\", mae)\n",
    "print(\"XGBoost - R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8d9a85",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning with GridSearchCV ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2e226602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}\n",
      "Mean Absolute Error: 11577.684041539635\n",
      "R-squared score: 0.9471523720329054\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 77;\n",
       "                var nbb_unformatted_code = \"from sklearn.model_selection import GridSearchCV\\nimport xgboost as xgb\\n\\n# Define the parameter grid\\nparam_grid = {\\n    'learning_rate': [0.1, 0.01],\\n    'max_depth': [3, 5, 7],\\n    'n_estimators': [100, 200, 500],\\n}\\n\\n# Initialize the XGBoost regressor\\nxgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\\n\\n# Initialize the GridSearchCV object\\ngrid_search = GridSearchCV(\\n    xgb_reg, \\n    param_grid, \\n    cv=5, \\n    scoring='neg_mean_absolute_error',\\n    n_jobs=-1\\n)\\n\\n# Fit the GridSearchCV object to the training data\\ngrid_search.fit(X_train, y_train)\\n\\n# Get the best hyperparameters and the corresponding model\\nbest_params = grid_search.best_params_\\nbest_xgb = grid_search.best_estimator_\\n\\n# Use the best model to make predictions on the test set\\ny_pred = best_xgb.predict(X_test)\\n\\n# Calculate the Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\n# Print the results\\nprint(\\\"Best Hyperparameters:\\\", best_params)\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_formatted_code = \"from sklearn.model_selection import GridSearchCV\\nimport xgboost as xgb\\n\\n# Define the parameter grid\\nparam_grid = {\\n    \\\"learning_rate\\\": [0.1, 0.01],\\n    \\\"max_depth\\\": [3, 5, 7],\\n    \\\"n_estimators\\\": [100, 200, 500],\\n}\\n\\n# Initialize the XGBoost regressor\\nxgb_reg = xgb.XGBRegressor(objective=\\\"reg:squarederror\\\", random_state=42)\\n\\n# Initialize the GridSearchCV object\\ngrid_search = GridSearchCV(\\n    xgb_reg, param_grid, cv=5, scoring=\\\"neg_mean_absolute_error\\\", n_jobs=-1\\n)\\n\\n# Fit the GridSearchCV object to the training data\\ngrid_search.fit(X_train, y_train)\\n\\n# Get the best hyperparameters and the corresponding model\\nbest_params = grid_search.best_params_\\nbest_xgb = grid_search.best_estimator_\\n\\n# Use the best model to make predictions on the test set\\ny_pred = best_xgb.predict(X_test)\\n\\n# Calculate the Mean Absolute Error (MAE) and R-squared score\\nmae = mean_absolute_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\n\\n# Print the results\\nprint(\\\"Best Hyperparameters:\\\", best_params)\\nprint(\\\"Mean Absolute Error:\\\", mae)\\nprint(\\\"R-squared score:\\\", r2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "}\n",
    "\n",
    "# Initialize the XGBoost regressor\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_reg, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE) and R-squared score\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6065e6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
